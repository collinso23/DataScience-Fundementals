{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--BOOK_INFORMATION-->\n",
    "<img align=\"left\" style=\"padding-right:10px;\" src=\"figures/PDSH-cover-small.png\">\n",
    "*This notebook contains an excerpt from the [Python Data Science Handbook](http://shop.oreilly.com/product/0636920034919.do) by Jake VanderPlas; the content is available [on GitHub](https://github.com/jakevdp/PythonDataScienceHandbook).*\n",
    "\n",
    "*The text is released under the [CC-BY-NC-ND license](https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode), and code is released under the [MIT license](https://opensource.org/licenses/MIT). If you find this content useful, please consider supporting the work by [buying the book](http://shop.oreilly.com/product/0636920034919.do)!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Google released its first AI Doodle on March, 2019\n",
    "\n",
    "[Sample doodle](https://www.google.com/doodles/celebrating-johann-sebastian-bach?doodle=80484806_h0xc7OKn7N4bWETVYvb9tdvg7ME1Jo0AX9Sv1gXsuUWvhVuGzB8XmBy33ldga3mKYR4p99Pd5GWL1Dsl3e8q0OWUn1-2KzU0d0179DLNK3qoK1cHUNhV-UD4ZsEi51u8u0V16u4cSKvebfhEhzaOW11MZXMoy_1HkCOBkSzgJkQkWCvYUJGInAn5uN8miMKb1B-8kPL_4HLebfEzIzXL97AC_8tw8ifSfwsWtfK4FLP5oimMDLd6-7eSm7ljFrfJa9gaGWpTyc9viMaRMYSZhyvSj67gRU26fyKN5rP8TEE4uDMm4XlTCx8PXOI0IdmVdmreB3-hSef_D9ic_DU6OAWACt7ovd-HU8gGQ2yi2WJnO3XSuMpQUNRiyEWU-wzYrbZke5hxLN665yvySBVqB7wIJ0vMXm8jyh5I6s6-QFbPTAt_W5PWIEkYVjcZZOoyq5EzT5g0UIQTHeH5M9uAXpM4cxzY2sWvljxC30pxBxrcMcjHoS7QXxGJkm9dsq_REvKmG1t93erU0Ugg3PIgMrCx7wVopcsanJLSmZk-HEOIh3fMrewoc6jJ6g6yuwpgn5JfeZMsQDAbosySSUGWXv21yqv04cf2-lUozMUrdKdB_or-bUXybzy-OFd785fWF2kiEQN7laBVc5RcoxSBRO-gkMxIqNEU2ZYK9iMyhMJTDUEFAW7JXJXF7uTJuQVo_AG4AxQJ5Vs1_TVVEZf4FltrOTHqKWrGQjrD_G9GuJIncN7U35Way0bNtvuyaT-c8aG96NcztMwFqaSPa1yhHnj9opCokPR1_oQWtqHu8Iwsg12WV2B_Qk4uco-1FQ60BSy2NF2V1qcz3f2H_xt_cjnEpW6BQJwDI2sVug8D4w0jWynMrNJU3PDe7ZKdsRzcQNzNTWtRHS9KCZnG6Q0yj1MFthRKqkWBl925J_NxD-a-7RLRliNnMJ7w8sNGRzQnPLPcFFB6oq0i-SB9LtqcSuHfUgfHD6dUTSjQhHpqrOUTnRv9YRfbjFowOmlajnQ5qHzGIVPxKzcVc0_Wql1PChBT-jf2m6AfKJoe0-HYXj0DKzVZrsu1oZ9GmtigtXfCtOq9gco0y0TtC0OGTXvDWJzfBie1lZYX9CMc_7bZdTkWjelbDiIEw0ocxvXtR_wGzPhtYvVlttIny65zuf4G9ktUj0oEq0uahejg3W-KYqGwbUjOpNlkJ_NAzAjaePdRvJ2waWLsJr_bPMokizKpae_3bY0jUkJApU6wtMIDeuKkEYepbtBIwZhW2ZjXfupXPM9HGxL_pSX_cefUi_qFJQ..&domain_name=google.com&hl=en)\n",
    "\n",
    "[Behind the Doodle](https://www.youtube.com/watch?v=XBfYPp6KF2g) (3min Youtube)\n",
    "\n",
    "[Bach-lash](https://slate.com/technology/2019/03/google-doodle-bach-ai-music-generator.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Submit \"Vizual Lab\" by next Thursday, 4/4 for credit. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\"When a computer creates art, who is the artist$-$the computer or the programmer? At MIT, a recent exhibit of highly accomplished algorithmic art had put an awkard spin on the Harvard humanities course: _Is Art What Makes Us Human?_\"\n",
    "\n",
    "Dan Brown _Origin_, p.86"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hyperparameters and Model Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### In the previous section, we saw the basic recipe for applying a supervised machine learning model:\n",
    "\n",
    "1. Choose a class of model\n",
    "2. Choose model hyperparameters\n",
    "3. Fit the model to the training data\n",
    "4. Use the model to predict labels for new data\n",
    "\n",
    "The first two pieces of this—the **choice of model and choice of hyperparameters**—are perhaps the most important part of using these tools and techniques effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In order to make an informed choice, we need a way to ** *validate* ** that our model and our hyperparameters are a **good fit to the data**.\n",
    "\n",
    "While this may sound simple, there are some pitfalls that you must avoid to do this effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Thinking about Model Validation\n",
    "\n",
    "In principle, model validation is very simple: after choosing a model and its hyperparameters, we can estimate how effective it is by **apply**ing it to some of the training data and **comparing the prediction to the known value**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The following sections first show a naive approach to model validation and why it fails, \n",
    "\n",
    "before exploring the use of **holdout sets and cross-validation** for more robust model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model validation the wrong way\n",
    "\n",
    "Let's demonstrate the naive approach to validation using the Iris data, which we saw in the previous section.\n",
    "We will start by loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Next we choose a model and hyperparameters. \n",
    "\n",
    "Here we'll use a ***k*-neighbors classifier with ``n_neighbors=1``**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is a very simple and intuitive model that says \"the label of an unknown point is the same as the label of its **closest training point**:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then we train the model, and use it to predict labels for data we already know:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(X, y)\n",
    "y_model = model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finally, we compute the fraction of correctly labeled points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y, y_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We see an accuracy score of 1.0, which indicates that **100% of points were correctly labeled** by our model!\n",
    "\n",
    "But is this truly measuring the expected accuracy? Have we really come upon a model that we expect to be correct 100% of the time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As you may have gathered, the answer is no.\n",
    "\n",
    "In fact, this approach contains a fundamental flaw: ***it trains and evaluates the model on the same data***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Furthermore, the nearest neighbor model is an **instance-based** estimator that simply stores the training data, and predicts labels by comparing new data to these stored points: except in contrived cases, it will get 100% accuracy *every time!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model validation the right way: Holdout sets\n",
    "\n",
    "So what can be done?\n",
    "\n",
    "A better sense of a model's performance can be found using what's known as a *holdout set*: that is, we hold back some subset of the data from the training of the model, and then use this **holdout set** to check the model performance.\n",
    "\n",
    "This splitting can be done using the ``train_test_split`` utility in Scikit-Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the data with 50% in each set\n",
    "X1, X2, y1, y2 = train_test_split(X, y, random_state=0,\n",
    "                                  train_size=0.5)\n",
    "print(X.size, X1.size, X2.size)\n",
    "\n",
    "# fit the model on one set of data\n",
    "model.fit(X1, y1)\n",
    "\n",
    "# evaluate the model on the second set of data\n",
    "y2_model = model.predict(X2)\n",
    "accuracy_score(y2, y2_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We see here a more reasonable result: the nearest-neighbor classifier is about 90% accurate on this hold-out set.\n",
    "\n",
    "The hold-out set is similar to unknown data, because the model has not \"seen\" it before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model validation via cross-validation\n",
    "\n",
    "One disadvantage of using a holdout set for model validation is that we have **lost a portion of our data** to the model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the above case, **half the dataset** does not contribute to the training of the model!\n",
    "\n",
    "This is not optimal, and can cause problems – especially if the initial set of training data is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One way to address this is to use ***cross-validation***; that is, to do a sequence of fits where **each subset of the data is used both as a training set and as a validation set**.\n",
    "\n",
    "Visually, it might look something like this:\n",
    "\n",
    "![](figures/05.03-2-fold-CV.png)\n",
    "[figure source in Appendix](06.00-Figure-Code.ipynb#2-Fold-Cross-Validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here we do **two validation trials**, alternately using each **half of the data** as a holdout set.\n",
    "\n",
    "Using the split data from before, we could implement it like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#train on first half. Test on second half\n",
    "y2_model = model.fit(X1, y1).predict(X2)\n",
    "#train on second half. Test on first half\n",
    "#Complete this line as an exercise. Solution below.\n",
    "# y1_model = ?\n",
    "\n",
    "accuracy_score(y1, y1_model), accuracy_score(y2, y2_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#train on first half. Test on second half\n",
    "y2_model = model.fit(X1, y1).predict(X2)\n",
    "#train on second half. Test on first half\n",
    "# *** Solution ***\n",
    "\n",
    "accuracy_score(y1, y1_model), accuracy_score(y2, y2_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What comes out are two accuracy scores, which we could combine (by, say, taking the **mean**) to get a better measure of the global model performance.\n",
    "\n",
    "This particular form of cross-validation is a ***two-fold cross-validation***—that is, one in which we have split the data into **two sets** and used each in turn as a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We could expand on this idea to use even more trials, and **more folds** in the data—for example, here is a visual depiction of **five-fold cross-validation**:\n",
    "\n",
    "![](figures/05.03-5-fold-CV.png)\n",
    "[figure source in Appendix](06.00-Figure-Code.ipynb#5-Fold-Cross-Validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here we split the data into five groups, and use each of them in turn to evaluate the model fit on the other 4/5 of the data.\n",
    "\n",
    "This would be rather tedious to do by hand, and so we can use Scikit-Learn's ``cross_val_score`` convenience routine to do it succinctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "\n",
    "print(scores)\n",
    "# mean score and the 95% confidence interval of the score estimate\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Repeating the validation across different subsets of the data gives us an even **better idea of the performance** of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Scikit-Learn implements a number of useful cross-validation schemes that are useful in particular situations; these are implemented via iterators in the ``cross_validation`` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For example, we might wish to go to the extreme case in which our **number of folds is equal to the number of data points**: that is, we train on all points but one in each trial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Each learning set is created by taking all the samples except one, the test set being the sample left out. \n",
    "\n",
    "Thus, for **n samples**, we have **n different training sets** and n different tests set. \n",
    "\n",
    "Con: Can be **very costly**\n",
    "\n",
    "This type of cross-validation is known as ***leave-one-out*** cross validation, and can be used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#older syntax. depreciating. See below for the newer syntax.\n",
    "from sklearn.cross_validation import LeaveOneOut\n",
    "scores = cross_val_score(model, X, y, cv=LeaveOneOut(len(X)))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#newer syntax\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np \n",
    "\n",
    "loo = LeaveOneOut()\n",
    "print(X.shape)\n",
    "print(loo.get_n_splits(X))\n",
    " \n",
    "\n",
    "#ME\n",
    "z=np.zeros(len(X))\n",
    "for train_index, test_index in loo.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    y_pred = model.fit(X_train, y_train).predict(X_test)\n",
    "    z[test_index] = accuracy_score(y_test, y_pred)   \n",
    "print(z)\n",
    "np.mean(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Because we have 150 samples, the leave one out cross-validation yields scores for 150 trials, and the score indicates either successful (1.0) or unsuccessful (0.0) prediction.\n",
    "\n",
    "Taking the mean of these gives an estimate of the error rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As a general rule, most authors, and empirical evidence, suggest that 5- or 10- fold cross validation should be preferred to LOO.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneOut.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Other cross-validation schemes can be used similarly.\n",
    "For a description of what is available in Scikit-Learn, use IPython to explore the ``sklearn.cross_validation`` submodule, or take a look at Scikit-Learn's online [cross-validation documentation](http://scikit-learn.org/stable/modules/cross_validation.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Selecting the Best Model\n",
    "\n",
    "Now that we've seen the basics of validation and cross-validation, we will go into a litte more depth regarding **model selection** and selection of **hyperparameters**.\n",
    "\n",
    "These issues are some of the most important aspects of the practice of machine learning, and I find that this information is often glossed over in introductory machine learning tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Of core importance is the following question: **if our estimator is underperforming, how should we move forward?**\n",
    "There are several possible answers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Use a more complicated/more flexible **model**\n",
    "- Use a less complicated/less flexible **model**\n",
    "- Gather more training **samples**\n",
    "- Gather more data to add **features** to each sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The answer to this question is often counter-intuitive.\n",
    "\n",
    "In particular, sometimes using a more complicated model will give worse results, and adding more training samples may not improve your results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The ability to determine what steps will improve your model is what separates the successful machine learning practitioners from the unsuccessful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Bias-variance trade-off\n",
    "\n",
    "Fundamentally, the question of \"the best model\" is about finding a sweet spot in the **tradeoff between *bias* and *variance*.**\n",
    "\n",
    "Consider the following figure, which presents two regression fits to the same dataset:\n",
    "\n",
    "![](figures/05.03-bias-variance.png)\n",
    "[figure source in Appendix](06.00-Figure-Code.ipynb#Bias-Variance-Tradeoff)\n",
    "\n",
    "1st vs 20th order polynomial fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It is clear that neither of these models is a particularly good fit to the data, but they fail in different ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The model on the **left** attempts to find a **straight-line** fit through the data.\n",
    "\n",
    "Because the data are intrinsically more complicated than a straight line, the straight-line model will never be able to describe this dataset well.\n",
    "\n",
    "Such a model is said to ***underfit*** the data: that is, it **does not have** enough model **flexibility** to suitably account for all the features in the data; another way of saying this is that the **model has high *bias***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The model on the **right** attempts to fit a **high-order polynomial** through the data.\n",
    "\n",
    "Here the model fit has enough flexibility to nearly perfectly account for the fine features in the data, but even though it very accurately describes the training data, its precise form seems to be more **reflective of** the particular **noise properties** of the data **rather than** the intrinsic properties of whatever process generated that **data**.\n",
    "\n",
    "Such a model is said to ***overfit*** the data: that is, it has **so much model flexibility** that the model ends up accounting for random errors as well as the underlying data distribution; another way of saying this is that the **model has high *variance***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To look at this in another light, consider what happens if we use these two models to **predict** the y-value for some **new data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the following diagrams, the **red/lighter points** indicate data that is **omitted** from the training set:\n",
    "\n",
    "![](figures/05.03-bias-variance-2.png)\n",
    "[figure source in Appendix](06.00-Figure-Code.ipynb#Bias-Variance-Tradeoff-Metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The score here is the $R^2$ score, or [coefficient of determination](https://en.wikipedia.org/wiki/Coefficient_of_determination), which measures **how well a model performs relative to a simple mean of the target** values. \n",
    "\n",
    "$R^2=1$ indicates a perfect match, $R^2=0$ indicates the model does no better than simply taking the mean of the data, and negative values mean even worse models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "From the scores associated with these two models, we can make an observation that holds more generally:\n",
    "\n",
    "- For **high-bias models**, the performance of the model on the validation set is **similar** to the performance on the training set.\n",
    "- For **high-variance models**, the performance of the model on the validation set is far **worse** than the performance on the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we imagine that we have some ability to tune the model complexity, we would expect the training score and validation score to behave as illustrated in the following figure:\n",
    "\n",
    "![](figures/05.03-validation-curve.png)\n",
    "[figure source in Appendix](06.00-Figure-Code.ipynb#Validation-Curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The diagram shown here is often called a ***validation curve***, and we see the following essential features:\n",
    "\n",
    "- The **training score** is everywhere **higher than** the **validation** score. This is generally the case: the model will be a better fit to data it has seen than to data it has not seen.\n",
    "- For very low model complexity (a **high-bias model**), the training data is **under-fit**, which means that the model is a poor predictor both for the training data and for any previously unseen data.\n",
    "- For very high model complexity (a **high-variance model**), the training data is **over-fit**, which means that the model predicts the training data very well, but fails for any previously unseen data.\n",
    "- For some intermediate value, the validation curve has a maximum. This level of complexity indicates a suitable **trade-off between bias and variance**.\n",
    "\n",
    "The means of tuning the model complexity varies from model to model; when we discuss individual models in depth in later sections, we will see how each model allows for such tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Validation curves in Scikit-Learn\n",
    "\n",
    "Let's look at an example of using cross-validation to compute the validation curve for a class of models.\n",
    "\n",
    "Here we will use a ***polynomial regression* model**: this is a generalized linear model in which the degree of the polynomial is a tunable parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For example, a degree-1 polynomial fits a straight line to the data; for model parameters $a$ and $b$:\n",
    "\n",
    "$$\n",
    "y = ax + b\n",
    "$$\n",
    "\n",
    "A degree-3 polynomial fits a cubic curve to the data; for model parameters $a, b, c, d$:\n",
    "\n",
    "$$\n",
    "y = ax^3 + bx^2 + cx + d\n",
    "$$\n",
    "\n",
    "We can generalize this to any number of polynomial features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In Scikit-Learn, we can implement this with a simple linear regression **combined with the polynomial preprocessor**.\n",
    "\n",
    "We will use a *pipeline* to string these operations together (we will discuss polynomial features and pipelines more fully in [Feature Engineering](05.04-Feature-Engineering.ipynb)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Construct a Pipeline from the given estimators.\n",
    "def PolynomialRegression(degree=2, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree),\n",
    "                         LinearRegression(**kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's create some data to which we will fit our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def make_data(N, err=1.0, rseed=1):\n",
    "    # randomly sample the data\n",
    "    rng = np.random.RandomState(rseed)\n",
    "    X = rng.rand(N, 1) ** 2\n",
    "    y = 10 - 1. / (X.ravel() + 0.1)\n",
    "    if err > 0:\n",
    "        y += err * rng.randn(N)\n",
    "    return X, y\n",
    "\n",
    "X, y = make_data(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can now visualize our data, along with polynomial fits of several degrees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn; seaborn.set()  # plot formatting\n",
    "\n",
    "X_test = np.linspace(-0.1, 1.1, 500)[:, None]\n",
    "\n",
    "plt.scatter(X.ravel(), y, color='black')\n",
    "axis = plt.axis()\n",
    "for degree in [1, 3, 5]:\n",
    "    y_test = PolynomialRegression(degree).fit(X, y).predict(X_test)\n",
    "    plt.plot(X_test.ravel(), y_test, label='degree={0}'.format(degree))\n",
    "plt.xlim(-0.1, 1.0)\n",
    "plt.ylim(-2, 12)\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The knob controlling model complexity in this case is the **degree of the polynomial**, which can be any non-negative integer.\n",
    "\n",
    "A useful question to answer is this: \n",
    "\n",
    "*what degree of polynomial provides a suitable trade-off between bias (under-fitting) and variance (over-fitting)*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can make progress in this by visualizing the validation curve for this particular data and model; this can be done straightforwardly using the ``validation_curve`` convenience routine provided by Scikit-Learn.\n",
    "\n",
    "Given a model, data, parameter name, and a range to explore, this function will automatically compute both the **training score and validation score** across the range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.learning_curve import validation_curve\n",
    "#newer syntax\n",
    "from sklearn.model_selection import validation_curve\n",
    "degree = np.arange(0, 21)\n",
    "#given an estimater and range for a param; returns train/valid scores\n",
    "train_score, val_score = validation_curve(PolynomialRegression(), X, y,\n",
    "                                          'polynomialfeatures__degree', degree, cv=7)\n",
    "\n",
    "plt.plot(degree, np.median(train_score, 1), color='blue', label='training score', marker='o')\n",
    "plt.plot(degree, np.median(val_score, 1), 'r-.', label='validation score')\n",
    "plt.legend(loc='best')\n",
    "plt.ylim(0, 1)\n",
    "plt.xlim(0, 21)\n",
    "plt.xlabel('degree')\n",
    "plt.ylabel('score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "This shows precisely the qualitative behavior we expect: \n",
    "- the training score is everywhere higher than the validation score; \n",
    "- the training score is monotonically improving with increased model complexity; \n",
    "- and the validation score reaches a maximum before dropping off as the model becomes over-fit.\n",
    "\n",
    "Where is the trade-off point? Which degree would you choose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "From the validation curve, we can read-off that the optimal trade-off between bias and variance is found for a third-order polynomial; we can compute and display this fit over the original data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X.ravel(), y)\n",
    "lim = plt.axis()\n",
    "y_test = PolynomialRegression(3).fit(X, y).predict(X_test)\n",
    "plt.plot(X_test.ravel(), y_test);\n",
    "plt.axis(lim);\n",
    "#accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Notice that finding this optimal model **did not** actually require us to **compute the training score**, but examining the relationship between the training score and validation score can give us useful insight into the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning Curves\n",
    "\n",
    "One important aspect of model complexity is that the optimal model will generally depend on the **size of your training data**.\n",
    "\n",
    "For example, let's generate a new dataset with a factor of five more points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X2, y2 = make_data(200)\n",
    "plt.scatter(X2.ravel(), y2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will duplicate the preceding code to **plot the validation curve** for this larger dataset; \n",
    "\n",
    "for reference let's over-plot the previous results as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "degree = np.arange(21)\n",
    "train_score2, val_score2 = validation_curve(PolynomialRegression(), X2, y2,\n",
    "                                            'polynomialfeatures__degree', degree, cv=7)\n",
    "#larger/new data: solid\n",
    "plt.plot(degree, np.median(train_score2, 1), color='blue', label='training score')\n",
    "plt.plot(degree, np.median(val_score2, 1), color='red', label='validation score')\n",
    "plt.plot(degree, np.median(train_score, 1), color='blue', alpha=0.3, linestyle='dashed', marker = 'o')\n",
    "plt.plot(degree, np.median(val_score, 1), color='red', alpha=0.3, linestyle='dashed')\n",
    "plt.legend(loc='lower center')\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel('degree')\n",
    "plt.ylabel('score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The solid lines show the new results, while the fainter dashed lines show the results of the previous smaller dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It is clear from the validation curve that the **larger dataset can support a much more complicated model**: the peak here is probably around a degree of 6, but even a degree-20 model is not seriously over-fitting the data—the validation and training scores remain very close."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Thus we see that the behavior of the validation curve has not one but two important inputs: \n",
    "- the model complexity and \n",
    "- the number of training points.\n",
    "\n",
    "It is often useful to to explore the behavior of the model as a function of the number of training points, which we can do by using increasingly larger subsets of the data to fit our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A plot of the training/validation score with respect to the size of the training set is known as a ***learning curve.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The general behavior we would expect from a learning curve is this:\n",
    "\n",
    "- A model of a given complexity will ***overfit* a small dataset**: this means the training score will be relatively high, while the validation score will be relatively low.\n",
    "- A model of a given complexity will ***underfit* a large dataset**: this means that the training score will decrease, but the validation score will increase.\n",
    "- A model will never, except by chance, give a better score to the validation set than the training set: this means the **curves should** keep getting closer together but **never cross**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "With these features in mind, we would expect a learning curve to look qualitatively like that shown in the following figure:\n",
    "\n",
    "[Instead of model score vs model complexity]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/05.03-learning-curve.png)\n",
    "[figure source in Appendix](06.00-Figure-Code.ipynb#Learning-Curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The notable feature of the learning curve is the convergence to a particular score as the number of training samples grows.\n",
    "\n",
    "In particular, once you have enough points that a particular **model has converged, *adding more training data will not help you!***\n",
    "\n",
    "The only way to increase model performance in this case is to use another (often more complex) model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning curves in Scikit-Learn\n",
    "\n",
    "Scikit-Learn offers a convenient utility for computing such learning curves from your models; \n",
    "\n",
    "here we will compute a learning curve for our original dataset with a second-order polynomial model and a ninth-order polynomial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.learning_curve import learning_curve\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "\n",
    "for i, degree in enumerate([2, 9]):\n",
    "    #sweep over training sizes. return corresponding scores\n",
    "    N, train_lc, val_lc = learning_curve(PolynomialRegression(degree),\n",
    "                                         X, y, cv=7,\n",
    "                                         train_sizes=np.linspace(0.3, 1, 25))\n",
    "\n",
    "    #Why do we take a mean here?\n",
    "    ax[i].plot(N, np.mean(train_lc, 1), color='blue', label='training score')\n",
    "    ax[i].plot(N, np.mean(val_lc, 1), color='red', label='validation score')\n",
    "    ax[i].hlines(np.mean([train_lc[-1], val_lc[-1]]), N[0], N[-1],\n",
    "                 color='gray', linestyle='dashed')\n",
    "\n",
    "    ax[i].set_ylim(0, 1)\n",
    "    ax[i].set_xlim(N[0], N[-1])\n",
    "    ax[i].set_xlabel('training size')\n",
    "    ax[i].set_ylabel('score')\n",
    "    ax[i].set_title('degree = {0}'.format(degree), size=14)\n",
    "    ax[i].legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(X.size, train_lc.size, val_lc.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is a valuable diagnostic, because it gives us a visual depiction of how our model responds to increasing training data.\n",
    "\n",
    "In particular, when your learning curve has already converged (i.e., when the training and validation curves are already close to each other) **adding more training data will not significantly improve the fit!**\n",
    "\n",
    "This situation is seen in the left panel, with the learning curve for the degree-2 model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The only way to increase the converged score is to use a different (usually more complicated) model.\n",
    "\n",
    "We see this in the right panel: by moving to a much more complicated model, we increase the score of convergence (indicated by the dashed line), but at the expense of higher model variance (indicated by the difference between the training and validation scores).\n",
    "\n",
    "If we were to add even more data points, the learning curve for the more complicated model would eventually converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Plotting a learning curve for your particular choice of model and dataset can help you to make this type of decision about how to move forward in improving your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Validation in Practice: Grid Search\n",
    "\n",
    "The preceding discussion is meant to give you some intuition into the **trade-off between bias and variance**, and its dependence on **model complexity and training set size**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In practice, models generally have **more than one knob to turn**, and thus plots of validation and learning curves change from lines to **multi-dimensional surfaces**.\n",
    "\n",
    "In these cases, such visualizations are difficult and we would rather simply find the **particular model that maximizes the validation score**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Scikit-Learn provides automated tools to do this in the grid search module.\n",
    "\n",
    "Here is an example of using **grid search to find the optimal polynomial model**.\n",
    "\n",
    "We will explore a three-dimensional grid of model features; namely the \n",
    "- polynomial degree, \n",
    "- the flag telling us whether to fit the intercept, and \n",
    "- the flag telling us whether to normalize the problem.\n",
    "\n",
    "This can be set up using Scikit-Learn's ``GridSearchCV`` meta-estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#set params as a dictionary\n",
    "param_grid = {'polynomialfeatures__degree': np.arange(21),\n",
    "              'linearregression__fit_intercept': [True, False],\n",
    "              'linearregression__normalize': [True, False]}\n",
    "\n",
    "#akin to training a model:\n",
    "grid = GridSearchCV(PolynomialRegression(), param_grid, cv=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Notice that like a normal estimator, this has not yet been applied to any data.\n",
    "\n",
    "Calling the ``fit()`` method will fit the model at each grid point, keeping track of the scores along the way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(X, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now that this is fit, we can ask for the best parameters as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finally, if we wish, we can use the best model and show the fit to our data using code from before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model = grid.best_estimator_\n",
    "\n",
    "plt.scatter(X.ravel(), y)\n",
    "lim = plt.axis()\n",
    "y_test = model.fit(X, y).predict(X_test)\n",
    "plt.plot(X_test.ravel(), y_test, hold=True);\n",
    "plt.axis(lim);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The grid search provides many more options, including the ability to specify a \n",
    "- custom scoring function, \n",
    "- to parallelize the computations, \n",
    "- to do randomized searches, and more.\n",
    "\n",
    "For information, see the examples in [In-Depth: Kernel Density Estimation](05.13-Kernel-Density-Estimation.ipynb) and [Feature Engineering: Working with Images](05.14-Image-Features.ipynb), or refer to Scikit-Learn's [grid search documentation](http://Scikit-Learn.org/stable/modules/grid_search.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hyperparameter tuning exercise\n",
    "\n",
    "Let's practice using the tools discussed in the last lecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Can we use grid search to improve our previous results?\n",
    "\n",
    "We will go back to digits classificaiton.\n",
    "\n",
    "We had managed to score 83% to 97% using NativeBayes and SVC.\n",
    "\n",
    "We will try to tune SVC() here to improve our score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# read the dataset, set vectors\n",
    "from sklearn.datasets import load_digits\n",
    "import numpy as np\n",
    "\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "print(X.shape)\n",
    "# print(X[:3])\n",
    "\n",
    "y = digits.target\n",
    "y.shape\n",
    "# print(y[:3])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Use GridSearch find the optimal set of parameters for SVC to: \n",
    "\n",
    "- Find the best kernel amongst: 'linear', 'poly', 'rbf', 'sigmoid'\n",
    "- For 'poly' try different degrees of fit: 1, 3, 5\n",
    "- Try different error term penalties: C=1,7\n",
    "- Should we enable shrinking? T or F\n",
    "\n",
    "- Use 5-fold cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# This is how we had tuned SVC last time. \n",
    "from sklearn.svm import SVC\n",
    "model = SVC(kernel='linear')\n",
    " \n",
    "#Exercise: \n",
    "\n",
    "# 1) Create a param_grid to tune SVC to the specs given before\n",
    "#See docstring or previous lecture for examples\n",
    "\n",
    "#2) performe grid search on the model\n",
    "\n",
    "#3) fit it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# %load ML_04_ex1.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Analyze the best parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "grid.best_params_\n",
    "# grid.cv_results_\n",
    "# sorted(grid.cv_results_.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Test our performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#2. Create a GaussianNB model based on training data\n",
    "model = grid.best_estimator_\n",
    "y_model = model.predict(Xtest)\n",
    "\n",
    "#3. Test it on test data\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(ytest, y_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can compare the performance visually. \n",
    "\n",
    "I like confusion matrices. Go back and compare the plots.\n",
    "\n",
    "We used to have 15 2s predicted as 8s..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "mat = confusion_matrix(ytest, y_model)\n",
    "sns.heatmap(mat, square=True, annot=True, cbar=False, cmap='YlGnBu') #flag, YlGnBu, jet\n",
    "plt.xlabel('predicted value')\n",
    "plt.ylabel('true value');\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this section, we have begun to explore the concept of **model validation and hyperparameter optimization**, focusing on intuitive aspects of the bias–variance trade-off and how it comes into play when fitting models to data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In particular, we found that the use of a **validation set or cross-validation** approach is ***vital*** when tuning parameters in order to avoid over-fitting for more complex/flexible models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Underfitting and overfitting:\n",
    "- the training score and validation score as a function of **model complexity**\n",
    "- the training score and validation score as a function of **training size**\n",
    "\n",
    "Grid search:\n",
    "- Seek for the model/hyperparameters that **maximizes the validation score**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In later sections, we will discuss the details of particularly useful models, and throughout will talk about what tuning is available for these models and how these free parameters affect model complexity.\n",
    "\n",
    "Keep the lessons of this section in mind as you read on and learn about these machine learning approaches!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
