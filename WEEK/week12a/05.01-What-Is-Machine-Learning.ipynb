{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<!--BOOK_INFORMATION-->\n",
    "*This notebook contains an excerpt from the [Python Data Science Handbook](http://shop.oreilly.com/product/0636920034919.do) by Jake VanderPlas; the content is available [on GitHub](https://github.com/jakevdp/PythonDataScienceHandbook).*\n",
    "\n",
    "*The text is released under the [CC-BY-NC-ND license](https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode), and code is released under the [MIT license](https://opensource.org/licenses/MIT). If you find this content useful, please consider supporting the work by [buying the book](http://shop.oreilly.com/product/0636920034919.do)!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What Is Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Before we take a look at the details of various machine learning methods, let's start by looking at **what machine learning is, and what it isn't**.\n",
    "\n",
    "Machine learning is often categorized as a **subfield of artificial intelligence**, but I find that categorization can often be misleading at first brush."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The study of machine learning certainly arose from research in this context, but in the data science application of machine learning methods, it's more helpful to think of machine learning as a means of **building models of data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Fundamentally, machine learning involves building mathematical models to help understand data.\n",
    "\"Learning\" enters the fray when we give these models **tunable parameters** that can be adapted to observed data; in this way the program can be considered to be \"learning\" from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Once these models have been fit to previously seen data, they can be used to **predict and understand aspects of newly observed data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "I'll leave to the reader the more philosophical digression regarding the extent to which this type of **mathematical, model-based \"learning\"** is similar to the **\"learning\" exhibited by the human brain**.\n",
    "\n",
    "Understanding the problem setting in machine learning is essential to using these tools effectively, and so we will start with some broad categorizations of the types of approaches we'll discuss here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Categories of Machine Learning\n",
    "\n",
    "At the most fundamental level, machine learning can be categorized into two main types: \n",
    "\n",
    "\n",
    "- supervised learning and \n",
    "- unsupervised learning\n",
    "- (not discussed) reinforcement learning, RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Supervised learning** involves somehow modeling the relationship between **measured features of data and some label associated with the data**; \n",
    "\n",
    "\n",
    "once this model is determined, it can be used to **apply labels to new, unknown data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is further subdivided into **classification** tasks and **regression** tasks: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "in **classification**, the labels are **discrete** categories, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "while in **regression**, the labels are **continuous** quantities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We will see examples of both types of supervised learning in the following section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Unsupervised learning** involves modeling the features of a dataset **without reference to any label**, and is often described as \"letting the dataset speak for itself.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "These models include tasks such as **clustering** and **dimensionality reduction.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Clustering algorithms identify distinct groups of data, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "while dimensionality reduction algorithms search for more succinct representations of the data.\n",
    "\n",
    "We will see examples of both types of unsupervised learning in the following section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In addition, there are so-called **semi-supervised learning** methods, which falls somewhere between supervised learning and unsupervised learning.\n",
    "\n",
    "Semi-supervised learning methods are often useful when only incomplete labels are available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Qualitative Examples of Machine Learning Applications\n",
    "\n",
    "To make these ideas more concrete, let's take a look at a few very simple examples of a machine learning task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "These examples are meant to give an intuitive, non-quantitative overview of the types of machine learning tasks we will be looking at in this chapter.\n",
    "\n",
    "In later sections, we will go into more depth regarding the particular models and how they are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For a preview of these more technical aspects, you can find the Python source that generates the following figures in the [Appendix: Figure Code](06.00-Figure-Code.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classification: Predicting discrete labels\n",
    "\n",
    "We will first take a look at a simple **classification** task, in which you are given a set of labeled points and want to use these to classify some unlabeled points.\n",
    "\n",
    "Imagine that we have the data shown in this figure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# common plot formatting for below\n",
    "def format_plot(ax, title):\n",
    "    ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "    ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "    ax.set_xlabel('feature 1', color='gray')\n",
    "    ax.set_ylabel('feature 2', color='gray')\n",
    "    ax.set_title(title, color='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "# create 50 separable points\n",
    "X, y = make_blobs(n_samples=50, n_features=2, centers=2,\n",
    "                  random_state=0, cluster_std=0.60)\n",
    "\n",
    "print(X[:3,:]); print(y[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(X.shape, y.shape)\n",
    "\n",
    "# plot the data\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(X[:, 0], X[:, 1], cmap='Paired', s=50)\n",
    "\n",
    "# format plot\n",
    "format_plot(ax, 'Input Data')\n",
    "ax.axis([-1, 4, -2, 7]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Let's make it prettier. \n",
    "\n",
    "# plot the data\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "#combine settings\n",
    "point_style = dict(c=y, cmap='Paired', s=50)\n",
    "ax.scatter(X[:, 0], X[:, 1], **point_style)\n",
    "#Note: * here is used for argument unpacking.\n",
    "#https://www.python.org/dev/peps/pep-0448/\n",
    "\n",
    "# format plot\n",
    "format_plot(ax, 'Input Data')\n",
    "ax.axis([-1, 4, -2, 7]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here we have two-dimensional data: that is, we have two **features** for each point, represented by the **(x,y)** positions of the points on the plane.\n",
    "\n",
    "In addition, we have one of two **class labels** for each point, here represented by the colors of the points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "From these features and labels, we would like to **create a model** that will let us decide whether a new point should be labeled **\"blue\" or \"red.\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are a **number of possible models** for such a classification task, but here we will use an extremely simple one. \n",
    "\n",
    "We will make the assumption that the two groups can be separated by drawing a **straight line** through the plane between them, such that points on each side of the line fall in the same group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Here the *model* is a quantitative version of the statement \"a straight line separates the classes\", while the ***model parameters*** are the particular numbers describing the **location and orientation of that line** for our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The optimal values for these model parameters are learned from the data (this is the \"learning\" in machine learning), which is often called **training the model**.\n",
    "\n",
    "The following figure shows a visual representation of what the trained model looks like for this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# fit the support vector classifier model\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X, y)\n",
    "\n",
    "#We have trained the model to find a line for our 2 classes.\n",
    "#We can apply our model to new data points using: clf.predict()\n",
    "\n",
    "#What did we learn?\n",
    "# print(clf.support_vectors_)\n",
    "# print(clf.decision_function)\n",
    "\n",
    "#Python calls the LIBSVM library which is written in C. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Get contours describing the model\n",
    "xx = np.linspace(-1, 4, 10)\n",
    "yy = np.linspace(-2, 7, 10)\n",
    "xy1, xy2 = np.meshgrid(xx, yy)\n",
    "Z = np.array([clf.decision_function([t])\n",
    "              for t in zip(xy1.flat, xy2.flat)]).reshape(xy1.shape)\n",
    "\n",
    "# plot points and model\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "line_style = dict(levels = [-1.0, 0.0, 1.0],\n",
    "                  linestyles = ['dashed', 'solid', 'dashed'],\n",
    "                  colors = 'gray', linewidths=1)\n",
    "ax.scatter(X[:, 0], X[:, 1], **point_style)\n",
    "ax.contour(xy1, xy2, Z, **line_style)\n",
    "\n",
    "# format plot\n",
    "format_plot(ax, 'Model Learned from Input Data')\n",
    "ax.axis([-1, 4, -2, 7])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now that this model has been trained, it can be generalized to new, unlabeled data.\n",
    "\n",
    "In other words, we can take a new set of data, draw this model line through it, and assign labels to the new points based on this model.\n",
    "\n",
    "This stage is usually called **prediction**. See the following figure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 1: Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#1. Create new data points using make_blobs, X2: 50 samples, but std is higher:0.8. \n",
    "# X2, _ = ?\n",
    "\n",
    "#2. Predict the labels\n",
    "# Instead of fitting; call the predict method and: \n",
    "# pass the new dataset, X2\n",
    "#call your predicted labels y2\n",
    "# y2 = ?\n",
    "\n",
    "# plot the results in 2 figures\n",
    "point_style = dict(cmap='Paired', s=50)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "\n",
    "#Plot the new dataset\n",
    "ax[0].scatter(X2[:, 0], X2[:, 1], c='gray', **point_style)\n",
    "ax[0].axis([-1, 4, -2, 7])\n",
    "#Plot the predicted labels\n",
    "ax[1].scatter(X2[:, 0], X2[:, 1], c=y2, **point_style)\n",
    "ax[1].contour(xy1, xy2, Z, **line_style)\n",
    "ax[1].axis([-1, 4, -2, 7])\n",
    "\n",
    "format_plot(ax[0], 'Unknown Data')\n",
    "format_plot(ax[1], 'Predicted Labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ML_01_ex1.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is the basic idea of a **classification** task in machine learning, where \"classification\" indicates that the data has **discrete class labels**.\n",
    "\n",
    "At first glance this may look fairly trivial: it would be relatively easy to simply look at this data and draw such a discriminatory line to accomplish this classification.\n",
    "\n",
    "A benefit of the machine learning approach, however, is that it can **generalize to much larger datasets** in many more dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For example, this is similar to the task of automated **spam detection for email**; in this case, we might use the following features and labels:\n",
    "\n",
    "- *feature 1*, *feature 2*, etc. $\\to$ normalized counts of important words or phrases (\"Viagra\", \"Nigerian prince\", etc.)\n",
    "- *label* $\\to$ \"spam\" or \"not spam\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For the **training set**, these labels might be determined by individual inspection of a small representative sample of emails; \n",
    "\n",
    "for the remaining emails, the label would be determined using the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For a suitably trained classification algorithm with enough well-constructed features (typically thousands or millions of words or phrases), this type of approach can be very effective.\n",
    "\n",
    "We will see an example of such text-based classification in 05.05-Naive-Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Some important classification algorithms that we will discuss in more detail are \n",
    "- Gaussian naive Bayes, \n",
    "- support vector machines, and \n",
    "- random forest classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regression: Predicting continuous labels\n",
    "\n",
    "In contrast with the discrete labels of a classification algorithm, we will next look at a simple **regression** task in which the labels are continuous quantities.\n",
    "\n",
    "Consider the data shown in the following figure, which consists of a set of points each with a continuous label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create some data for the regression\n",
    "rng = np.random.RandomState(1)\n",
    "\n",
    "#Features are random\n",
    "X = rng.randn(200, 2)\n",
    "#output is a function of these features\n",
    "y = np.dot(X, [-2, 1]) + 0.1 * rng.randn(X.shape[0])\n",
    "print(X[:5,:],'\\n y=',y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# plot data points\n",
    "fig, ax = plt.subplots()\n",
    "points = ax.scatter(X[:, 0], X[:, 1], c=y, s=50,\n",
    "                    cmap='viridis')\n",
    "\n",
    "# format plot\n",
    "format_plot(ax, 'Input Data')\n",
    "ax.axis([-4, 4, -3, 3])\n",
    "\n",
    "print(X[:3,:]); print(y[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As with the classification example, we have two-dimensional data: that is, there are two features describing each data point.\n",
    "\n",
    "The **color** of each point represents the **continuous label** for that point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are a number of possible regression models we might use for this type of data, but here we will use a simple linear regression to predict the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# fit the regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This simple linear regression model assumes that if we treat the label as a third spatial dimension, we can fit a plane to the data.\n",
    "\n",
    "This is a higher-level generalization of the well-known problem of fitting a line to data with two coordinates.\n",
    "\n",
    "We can visualize this setup as shown in the following figure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://github.com/jakevdp/PythonDataScienceHandbook/raw/master/notebooks/figures/05.01-regression-2.png)\n",
    "[figure source in Appendix](06.00-Figure-Code.ipynb#Regression-Example-Figure-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Notice that the **feature 1-feature 2** plane here is the same as in the two-dimensional plot from before; in this case, however, we have represented the labels by both color and three-dimensional axis position.\n",
    "\n",
    "From this view, it seems reasonable that fitting a plane through this three-dimensional data would allow us to predict the expected label for any set of input parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Returning to the two-dimensional projection, when we fit such a plane we get the result shown in the following figure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "# plot data points\n",
    "fig, ax = plt.subplots()\n",
    "pts = ax.scatter(X[:, 0], X[:, 1], c=y, s=50,\n",
    "                 cmap='viridis', zorder=2)\n",
    "\n",
    "# compute and plot model color mesh\n",
    "xx, yy = np.meshgrid(np.linspace(-4, 4),\n",
    "                     np.linspace(-3, 3))\n",
    "Xfit = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "#fit data to model\n",
    "yfit = model.predict(Xfit)\n",
    "zz = yfit.reshape(xx.shape)\n",
    "ax.pcolorfast([-4, 4], [-3, 3], zz, alpha=0.5,\n",
    "              cmap='viridis', norm=pts.norm, zorder=1)\n",
    "\n",
    "# format plot\n",
    "format_plot(ax, 'Input Data with Linear Fit')\n",
    "ax.axis([-4, 4, -3, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This plane of fit gives us what we need to predict labels for new points.\n",
    "Visually, we find the results shown in the following figure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 2: Predict regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#1. Create new data points, X2: 100 samples, 2 features. use rng \n",
    "# X2 = ?\n",
    "\n",
    "#2. Predict the labels\n",
    "# Instead of fitting; call the predict method and pass the new dataset, X2\n",
    "#call your predicted labels y2\n",
    "# y2 = ?\n",
    "\n",
    "print(y2[:3])\n",
    "\n",
    "# plot the model fit\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "\n",
    "ax[0].scatter(X2[:, 0], X2[:, 1], c='gray', s=50)\n",
    "ax[0].axis([-4, 4, -3, 3])\n",
    "\n",
    "ax[1].scatter(X2[:, 0], X2[:, 1], c=y2, s=50,\n",
    "              cmap='viridis', norm=pts.norm)\n",
    "ax[1].axis([-4, 4, -3, 3])\n",
    "\n",
    "# format plots\n",
    "format_plot(ax[0], 'Unknown Data')\n",
    "format_plot(ax[1], 'Predicted Labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To see the solution, load or open the following file:\n",
    "# %load ML_01_ex2.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As with the classification example, this may seem rather trivial in a low number of dimensions.\n",
    "\n",
    "But the power of these methods is that they can be straightforwardly applied and evaluated in the case of data with many, many features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For example, this is similar to the task of computing the **distance to galaxies observed through a telescope**â€”in this case, we might use the following features and labels:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- *feature 1*, *feature 2*, etc. $\\to$ brightness of each galaxy at one of several wave lengths or colors\n",
    "- *label* $\\to$ distance or redshift of the galaxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The distances for a small number of these galaxies might be determined through an independent set of (typically more expensive) observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Distances to remaining galaxies could then be estimated using a suitable regression model, without the need to employ the more expensive observation across the entire set.\n",
    "\n",
    "In astronomy circles, this is known as the \"photometric redshift\" problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Some important **regression algorithms** that we will discuss are \n",
    "- linear regression,\n",
    "- support vector machines, and \n",
    "- random forest regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Clustering: Inferring labels on unlabeled data\n",
    "\n",
    "The classification and regression illustrations we just looked at are examples of supervised learning algorithms, in which we are trying to build a model that will predict labels for new data.\n",
    "\n",
    "**Unsupervised learning** involves models that describe data without reference to any known labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One common case of unsupervised learning is \"clustering,\" in which data is automatically assigned to some number of discrete groups.\n",
    "\n",
    "For example, we might have some two-dimensional data like that shown in the following figure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "# create 50 separable points\n",
    "X, y = make_blobs(n_samples=100, centers=4, \n",
    "                  random_state=42, cluster_std=1.5)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# plot the input data\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(X[:, 0], X[:, 1], s=50, color='gray')\n",
    "\n",
    "# format the plot\n",
    "format_plot(ax, 'Input Data')\n",
    "\n",
    "#How many \"groups\" of data do we have? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Clustering: Inferring labels on unlabeled data\n",
    "\n",
    "The classification and regression illustrations we just looked at are examples of supervised learning algorithms, in which we are trying to build a model that will predict labels for new data.\n",
    "\n",
    "**Unsupervised learning** involves models that describe data without reference to any known labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One common case of unsupervised learning is \"clustering,\" in which **data is automatically assigned to some number of discrete groups**.\n",
    "\n",
    "For example, we might have some two-dimensional data like that shown in the following figure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "# create 50 separable points\n",
    "X, y = make_blobs(n_samples=100, centers=4,\n",
    "                  random_state=42, cluster_std=1.5)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# plot the input data\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(X[:, 0], X[:, 1], s=50, color='gray')\n",
    "\n",
    "# format the plot\n",
    "format_plot(ax, 'Input Data')\n",
    "\n",
    "#How many \"groups\" of data do we have? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By eye, it is clear that each of these points is part of a distinct group.\n",
    "\n",
    "Given this input, a **clustering model** will use the intrinsic structure of the data to determine which points are related.\n",
    "\n",
    "Using the very fast and intuitive ***k*-means algorithm**, we find the clusters shown in the following figure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Fit the K Means model\n",
    "model = KMeans(4, random_state=0)\n",
    "y = model.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# plot the data with cluster labels\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(X[:, 0], X[:, 1], s=50, c=y, cmap='viridis')\n",
    "\n",
    "# format the plot\n",
    "format_plot(ax, 'Learned Cluster Labels')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***k*-means** fits a model consisting of ***k* cluster centers**; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "the optimal centers are assumed to be those that minimize the distance of each point from its assigned center."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Again, this might seem like a trivial exercise in two dimensions, but as our data becomes larger and more complex, such clustering algorithms can be employed to extract useful information from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will discuss the *k*-means algorithm in more depth in 05.11-K-Means.ipynb.\n",
    "Other important clustering algorithms include \n",
    "- Gaussian mixture models and \n",
    "- spectral clustering (See [Scikit-Learn's clustering documentation](http://scikit-learn.org/stable/modules/clustering.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dimensionality reduction: Inferring structure of unlabeled data\n",
    "\n",
    "Dimensionality reduction is another example of an **unsupervised algorithm**, in which labels or other information are inferred from the structure of the dataset itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Dimensionality reduction is a bit more abstract than the examples we looked at before, but generally it seeks to pull out some **low-dimensional representation of data** that in some way **preserves relevant qualities** of the full dataset.\n",
    "\n",
    "Different dimensionality reduction routines measure these relevant qualities in different ways, as we will see in 05.10-Manifold-Learning.ipynb.\n",
    "\n",
    "As an example of this, consider the data shown in the following figure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_swiss_roll\n",
    "\n",
    "# make data\n",
    "X, y = make_swiss_roll(200, noise=0.5, random_state=42)\n",
    "X = X[:, [0, 2]]\n",
    "\n",
    "# visualize data\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:, 0], X[:, 1], color='gray', s=30)\n",
    "\n",
    "# format the plot\n",
    "format_plot(ax, 'Input Data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Visually, it is clear that there is some **structure in this data**: it is drawn from a one-dimensional line that is arranged in a spiral within this two-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In a sense, you could say that this data is **\"intrinsically\" only one dimensional**, though this one-dimensional data is embedded in higher-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A suitable dimensionality reduction model in this case would be sensitive to this nonlinear embedded structure, and be able to **pull out this lower-dimensionality representation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The following figure shows a visualization of the results of the Isomap algorithm, a manifold learning algorithm that does exactly this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap\n",
    "\n",
    "model = Isomap(n_neighbors=8, n_components=1)\n",
    "y_fit = model.fit_transform(X).ravel()\n",
    "\n",
    "# visualize data\n",
    "fig, ax = plt.subplots()\n",
    "pts = ax.scatter(X[:, 0], X[:, 1], c=y_fit, cmap='viridis', s=30)\n",
    "cb = fig.colorbar(pts, ax=ax)\n",
    "\n",
    "# format the plot\n",
    "format_plot(ax, 'Learned Latent Parameter')\n",
    "cb.set_ticks([])\n",
    "cb.set_label('Latent Variable', color='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Notice that the colors (which represent the extracted one-dimensional latent variable) change uniformly along the spiral, which indicates that the algorithm did in fact detect the structure we saw by eye.\n",
    "\n",
    "As with the previous examples, the power of dimensionality reduction algorithms becomes clearer in higher-dimensional cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For example, we might wish to visualize important relationships within a dataset that has 100 or 1,000 features.\n",
    "\n",
    "**Visualizing 1,000-dimensional data is a challenge**, and one way we can make this more manageable is to use a dimensionality reduction technique to **reduce the data to two or three dimensions**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Some important dimensionality reduction algorithms that we will discuss are \n",
    "- principal component analysis (see 05.09-Principal-Component-Analysis.ipynb) and \n",
    "- various manifold learning algorithms, including Isomap and locally linear embedding (See 05.10-Manifold-Learning.ipynb)\n",
    "- linear discriminant analysis (see [Wiki])(https://en.wikipedia.org/wiki/Linear_discriminant_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "Here we have seen a few simple examples of some of the basic types of machine learning approaches.\n",
    "Needless to say, there are a number of important practical details that we have glossed over, but I hope this section was enough to give you a basic idea of what types of problems machine learning approaches can solve.\n",
    "\n",
    "In short, we saw the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- *Supervised learning*: Models that can predict labels based on labeled training data\n",
    "\n",
    "  - *Classification*: Models that predict labels as two or more discrete categories\n",
    "  - *Regression*: Models that predict continuous labels\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- *Unsupervised learning*: Models that identify structure in unlabeled data\n",
    "\n",
    "  - *Clustering*: Models that detect and identify distinct groups in the data\n",
    "  - *Dimensionality reduction*: Models that detect and identify lower-dimensional structure in higher-dimensional data\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the following sections we will go into much greater depth within these categories, and see some more interesting examples of where these concepts can be useful.\n",
    "\n",
    "All of the figures in the preceding discussion are generated based on actual machine learning computations; the code behind them can be found in [Appendix: Figure Code](06.00-Figure-Code.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
